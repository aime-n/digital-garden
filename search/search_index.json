{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Digital Garden","text":"<p>Hey. This isn't quite a blog, and none of this is the final version. Welcome.</p>"},{"location":"Computer%20Science/Computational%20Complexity/","title":"Computational Complexity","text":""},{"location":"Computer%20Science/Computational%20Complexity/#computational-complexity","title":"Computational Complexity","text":""},{"location":"Computer%20Science/Computational%20Complexity/#introduction","title":"Introduction","text":"<p>Computational complexity refers to the quantitative measure used to describe the amount of computational time and resources consumed by an algorithm. The two primary forms of complexity are time complexity and space complexity.</p>"},{"location":"Computer%20Science/Computational%20Complexity/#time-complexity","title":"Time Complexity","text":""},{"location":"Computer%20Science/Computational%20Complexity/#definition","title":"Definition","text":"<ul> <li>Time complexity represents the amount of time an algorithm takes to run as a function of the length of the input.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#big-o-notation","title":"Big O Notation","text":"<ul> <li>The most common notation to represent time complexity is the Big O notation. It describes the upper bound of an algorithm's running time.</li> <li>Examples include \\(O(1)\\), \\(O(N)\\), \\(O(\\log N)\\), \\(O(N^2)\\), \\(O(N \\log N)\\), etc.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#time-complexities-overview","title":"Time Complexities Overview:","text":"<ul> <li>Constant Time: \\(O(1)\\)</li> <li> <p>The algorithm takes a constant amount of time, irrespective of the input size.</p> </li> <li> <p>Logarithmic Time: \\(O(\\log N)\\)</p> </li> <li> <p>The algorithm takes time proportional to the logarithm of the input size.</p> </li> <li> <p>Linear Time: \\(O(N)\\)</p> </li> <li> <p>The algorithm's runtime increases linearly with the input size.</p> </li> <li> <p>Linear Logarithmic Time: \\(O(N \\log N)\\)</p> </li> <li> <p>Common in algorithms that divide the input in each step, like many sorting algorithms.</p> </li> <li> <p>Polynomial Time: \\(O(N^k)\\)</p> </li> <li> <p>Where \\(k\\) is a constant. Polynomial-time algorithms become impractical for large inputs.</p> </li> <li> <p>Exponential Time: \\(O(2^N)\\), \\(O(k^N)\\)</p> </li> <li>The algorithm's runtime doubles with each additional element in the input. These algorithms are often impractical for even small inputs.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#space-complexity","title":"Space Complexity","text":""},{"location":"Computer%20Science/Computational%20Complexity/#definition_1","title":"Definition","text":"<ul> <li>Space complexity represents the amount of memory space an algorithm takes relative to the input size.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#big-o-notation-for-space","title":"Big O Notation for Space","text":"<ul> <li>Similar to time complexity, space complexity is often expressed using Big O notation, like \\(O(1)\\), \\(O(N)\\), \\(O(N^2)\\), etc.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#space-complexities-overview","title":"Space Complexities Overview:","text":"<ul> <li>Constant Space: \\(O(1)\\)</li> <li> <p>The algorithm uses a fixed amount of space.</p> </li> <li> <p>Linear Space: \\(O(N)\\)</p> </li> <li>The algorithm's space requirement grows linearly with the input size.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#complexity-classes","title":"Complexity Classes","text":"<ul> <li>P-Class:</li> <li> <p>Algorithms that can be solved in polynomial time.</p> </li> <li> <p>NP-Class:</p> </li> <li> <p>Non-deterministic Polynomial time. Problems where a solution, once guessed, can be verified quickly.</p> </li> <li> <p>NP-Complete:</p> </li> <li> <p>The hardest problems in NP-class, to which all NP problems can be reduced.</p> </li> <li> <p>NP-Hard:</p> </li> <li>Problems that are at least as hard as the hardest problems in NP-class.</li> </ul>"},{"location":"Computer%20Science/Computational%20Complexity/#conclusion","title":"Conclusion","text":"<p>Understanding computational complexity is crucial for designing efficient algorithms and systems. Time and space complexities provide a high-level understanding of an algorithm\u2019s efficiency and help developers make informed choices about which algorithms to use under different circumstances.</p>"},{"location":"Computer%20Science/Data%20Structure/","title":"Data Structure","text":""},{"location":"Computer%20Science/Data%20Structure/#mini-wiki-data-structures","title":"Mini Wiki: Data Structures","text":""},{"location":"Computer%20Science/Data%20Structure/#introduction","title":"Introduction","text":"<p>Data structures are specialized formats for organizing, storing, and manipulating data on a computer. They are essential for efficient data retrieval and manipulation, enabling developers to write efficient algorithms.</p>"},{"location":"Computer%20Science/Data%20Structure/#fundamental-data-structures","title":"Fundamental Data Structures","text":""},{"location":"Computer%20Science/Data%20Structure/#1-arrays","title":"1. Arrays:","text":"<ul> <li>Definition: Fixed-size, sequential collection of elements of the same type.</li> <li>Use Cases: Quick access to elements based on index, small and constant size lists.</li> <li>Complexity: Constant time access, linear time insertion and deletion.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#2-linked-lists","title":"2. Linked Lists:","text":"<ul> <li>Definition: Collection of nodes where each node contains a value and a reference to the next node in the sequence.</li> <li>Use Cases: Dynamic size lists, constant time insertions and deletions.</li> <li>Types: Singly Linked List, Doubly Linked List, Circular Linked List.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#3-stacks","title":"3. Stacks:","text":"<ul> <li>Definition: Collection of elements with Last In, First Out (LIFO) principle.</li> <li>Use Cases: Undo mechanisms, parsing expressions, depth-first search.</li> <li>Operations: Push, Pop, Peek.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#4-queues","title":"4. Queues:","text":"<ul> <li>Definition: Collection of elements with First In, First Out (FIFO) principle.</li> <li>Use Cases: Order processing, breadth-first search.</li> <li>Types: Simple Queue, Circular Queue, Priority Queue, Dequeue.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#5-hash-tables","title":"5. Hash Tables:","text":"<ul> <li>Definition: Collection of key-value pairs with a hash function mapping keys to indices.</li> <li>Use Cases: Quick lookups, de-duplication, frequency counting.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#6-trees","title":"6. Trees:","text":"<ul> <li>Definition: Hierarchical data structure with a root element and connected nodes.</li> <li>Use Cases: Hierarchical data representation, sorting, searching.</li> <li>Types: Binary Tree, Binary Search Tree, AVL Tree, Red-Black Tree.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#7-graphs","title":"7. Graphs:","text":"<ul> <li>Definition: Collection of nodes (vertices) connected by edges.</li> <li>Use Cases: Network representation, social networks, routing algorithms.</li> <li>Types: Directed, Undirected, Weighted, Unweighted.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#8-matrices","title":"8. Matrices:","text":"<ul> <li>Definition: Two-dimensional array with rows and columns.</li> <li>Use Cases: Graph representation, image processing, mathematical calculations.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#advanced-data-structures","title":"Advanced Data Structures:","text":""},{"location":"Computer%20Science/Data%20Structure/#1-heaps","title":"1. Heaps:","text":"<ul> <li>Definition: Specialized tree-based structure satisfying the heap property.</li> <li>Use Cases: Priority queues, heap sort.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#2-tries","title":"2. Tries:","text":"<ul> <li>Definition: Tree-like structure used for storing a dynamic set of strings.</li> <li>Use Cases: Auto-complete suggestions, IP routing.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#3-b-trees-and-b-trees","title":"3. B-Trees and B+ Trees:","text":"<ul> <li>Definition: Balanced tree data structure maintaining sorted data and allowing searches, insertions, and deletions.</li> <li>Use Cases: File systems, databases.</li> </ul>"},{"location":"Computer%20Science/Data%20Structure/#conclusion","title":"Conclusion","text":"<p>Data structures are foundational to computer science and programming, playing a crucial role in designing efficient software. Understanding when and how to use different data structures is key to writing efficient and effective algorithms and software solutions. Each data structure offers specific advantages and is suited to particular tasks, and choosing the right one for the job is a critical skill for developers and programmers.</p>"},{"location":"Personal%20wiki/Digital%20Garden/","title":"Digital Garden","text":"<p>I had this idea today and its main purpose is several: - to remember since I'm have such a bad memory; - write faster, I think it's sexy; - Retrieval information - exercise my mind, and; - learn English.</p>"},{"location":"Personal%20wiki/Digital%20Garden/#todo","title":"Todo","text":"<ul> <li> find how to set a git page using obsidian layout \u2705 2023-09-15         Digital Garden Repo GitHub - jobindjohn/obsidian-publish-mkdocs: A Template to Publish Obsidian/Foam Notes on Github Pages (uses MkDocs)</li> <li> [[login in all dispositives]] \u2705 2023-09-14</li> <li> write and set templates</li> <li> download plugins</li> <li> chose anoter theme for the web version</li> </ul>"},{"location":"Personal%20wiki/Digital%20Garden/#subjetcs","title":"Subjetcs","text":"<ul> <li>Statistics</li> <li>Movies</li> <li>Books</li> <li>Music</li> <li>Computer Science<ul> <li>NLP</li> </ul> </li> </ul> <p>Fetching Title#qf45</p> <p>MindStone is a free open-source alternative solution to\u00a0Obsidian Publish GitHub - TuanManhCao/digital-garden: Free Obisidian Publish alternative, for publishing your digital garden.</p> <p>Develop and deploy websites and apps in record time | Netlify Material for MkDocs</p> <p>The Everything App</p> <p>The Everything App</p> <p>GitHub - binnyva/gatsby-garden: A Digital Garden Theme for Gatsby. Gatsby Garden lets you create a static HTML version of your markdown notes</p> <p>Markbase</p> <p>Material for MkDocs</p> <p>Markdown Options | Jekyll \u2022 Simple, blog-aware, static sites</p>"},{"location":"Personal%20wiki/Digital%20Garden/#see-more","title":"See more","text":"<p>Retrieval vs Recall</p> <p>Diagrams - Material for MkDocs</p>"},{"location":"Personal%20wiki/Retrieval%20vs%20Recall/","title":"Retrieval vs Recall","text":"<p>[!info] - Recall:    - def: Recall is often a component of the retrieval process. The ability to remember and reproduce information   - example: recall experiences, like the history of a book   - ptbr: Lembran\u00e7a ou recorda\u00e7\u00e3o - Retrieval: The process of actively searching for and accessing stored information from memory.   - example: search on database or flash card   - ptbr: recupera\u00e7\u00e3o ou acesso</p> <p>Memory retrieval is the process of remembering information stored in long-term memory. There are three main types of memory retrieval: recall, recognition and relearning.\u00a0Recall occurs when the information must be retrieved from memories.</p>"},{"location":"Personal%20wiki/Wet%20Bulb/","title":"Wet Bulb","text":"<p>Wet Bulb Temperature and Its Implications for Climate</p> <p>The wet bulb temperature is a critical concept in meteorology and climatology, with profound implications for human health and ecosystems in a warming world. In this article, we'll delve into what wet bulb temperature is, its significance, and address some of the primary concerns surrounding it.</p>"},{"location":"Personal%20wiki/Wet%20Bulb/#1-definition","title":"1. Definition","text":"<ul> <li>Wet Bulb Temperature: The lowest temperature to which air can be cooled by evaporating water into it at constant atmospheric pressure, without adding or removing heat. It's determined using a thermometer wrapped in a water-soaked cloth. As the water evaporates, it cools the thermometer to the wet bulb temperature.</li> </ul>"},{"location":"Personal%20wiki/Wet%20Bulb/#2-significance","title":"2. Significance","text":"<ul> <li>Human Health: The human body primarily cools itself through sweating. If air temperature and humidity are so high that sweat evaporation becomes inefficient or impossible, the body can't cool effectively, potentially leading to fatal overheating. Conditions with wet bulb temperatures exceeding 35\u00b0C (95\u00b0F) are intolerable for prolonged human exposure.</li> </ul>"},{"location":"Personal%20wiki/Wet%20Bulb/#3-common-questions","title":"3. Common Questions","text":"<ul> <li>How does wet bulb temperature differ from ambient temperature?</li> <li> <p>The wet bulb temperature is always equal to or lower than the air temperature, with the difference depending on humidity. The higher the humidity, the closer the wet bulb temperature will be to the ambient temperature.</p> </li> <li> <p>Why are wet bulb temperatures above 35\u00b0C dangerous?</p> </li> <li> <p>Even if the ambient temperature doesn't seem excessively hot, high humidity can make conditions lethal due to the body's inability to cool itself through sweat evaporation.</p> </li> <li> <p>Can global warming lead to more frequent and intense wet bulb temperature events?</p> </li> <li> <p>There is growing concern that global warming may lead to more frequent and intense wet bulb temperature events, especially in tropical and subtropical regions.</p> </li> <li> <p>What's the relationship between wet bulb temperature and dew point?</p> </li> <li>The dew point is the temperature at which air becomes saturated with water vapor, and water begins to condense. The wet bulb temperature always falls between the ambient temperature and the dew point.</li> </ul>"},{"location":"Personal%20wiki/Wet%20Bulb/#4-conclusion","title":"4. Conclusion","text":"<p>Understanding and monitoring wet bulb temperatures is crucial in a climatically changing world. These temperatures have direct implications on humans' ability to endure and adapt to extreme conditions, making them pivotal in long-term planning and mitigation strategies.</p>"},{"location":"Philosofy/Julia%20Kristeva/","title":"Julia Kristeva","text":"","tags":["#person","#psychoalanysis","France"]},{"location":"Philosofy/Julia%20Kristeva/#notes","title":"Notes","text":"<ul> <li>Found in the Book of Philosophy</li> </ul>","tags":["#person","#psychoalanysis","France"]},{"location":"Philosofy/Julia%20Kristeva/#interests","title":"Interests","text":"","tags":["#person","#psychoalanysis","France"]},{"location":"Philosofy/Julia%20Kristeva/#semiotics","title":"Semiotics","text":"<p>She developed the idea of \"semanalysis,\" a term she coined to describe her approach to analyzing the ways in which language and symbols shape human consciousness and culture.</p>","tags":["#person","#psychoalanysis","France"]},{"location":"Philosofy/Julia%20Kristeva/#feminism","title":"[[Feminism]]","text":"<p>Julia Kristeva's relationship with feminism is complex, and her position within feminist theory has been a subject of debate and discussion. While she is associated with feminism due to her contributions to the understanding of women's experiences and her exploration of gender-related topics, her feminist stance is not straightforward, and she has sometimes been critiqued by feminists for various reasons. Here are some key aspects of Julia Kristeva's engagement with feminism:</p> <ol> <li> <p>Exploration of Women's Experiences: Kristeva's work often delves into issues related to the female experience, motherhood, and women's roles in society. In her writings, she explores the complexities of women's identities, particularly in the context of psychoanalysis and literature. Her interest in women's experiences has resonated with feminist scholars who appreciate her attention to these topics.</p> </li> <li> <p>Critique of Traditional Feminism: Kristeva has been critical of certain aspects of traditional or mainstream feminism. She has expressed reservations about some feminist movements' tendency to reduce women to a singular, fixed identity and has questioned the idea of a universal \"woman.\" Her work suggests that the experience of womanhood is multifaceted and influenced by various factors.</p> </li> <li> <p>Emphasis on Ambiguity and Fluidity: Kristeva's writings often emphasize ambiguity, fluidity, and the blurring of boundaries. She challenges binary notions of gender and identity, arguing for a more nuanced and complex understanding. This perspective aligns with poststructuralist and postmodern feminist thought, which also questions fixed categories.</p> </li> <li> <p>Abjection and the Maternal: Kristeva's concept of \"abjection\" explores the uncomfortable and often taboo aspects of human existence, including the maternal body. While her analysis of the maternal can be seen as a feminist critique of the way motherhood is often idealized or marginalized, it has also been critiqued for reinforcing traditional gender roles.</p> </li> <li> <p>Interdisciplinary Approach: Kristeva's work bridges the gap between psychoanalysis, linguistics, literature, and cultural studies, offering a unique interdisciplinary perspective on gender and identity. This approach has enriched feminist scholarship by drawing from diverse fields.</p> </li> </ol> <p>In summary, Julia Kristeva's relationship with feminism is complex and multifaceted. While she has made significant contributions to feminist theory by exploring women's experiences and challenging traditional gender norms, her work has also been critiqued by feminists who find her ideas challenging or who see her as departing from certain feminist orthodoxies. Ultimately, her work has had a notable impact on feminist discourse by introducing nuanced and multidimensional perspectives on gender, identity, and subjectivity.</p>","tags":["#person","#psychoalanysis","France"]},{"location":"Philosofy/Semiotics/","title":"Semiotics","text":"<p>Semiotics is the study of signs, symbols, and the processes of meaning-making. It is a field of study that examines how humans create, interpret, and communicate meaning through various forms of signs and symbols, including language, gestures, images, and more.</p> <p>Key concepts in semiotics include:</p> <ul> <li> <p>Sign: In semiotics, a sign is a basic unit of analysis. A sign consists of two components: a signifier (the form or representation of the sign) and a signified (the concept or meaning associated with the sign). For example, the word \"apple\" (signifier) represents the fruit (signified).</p> </li> <li> <p>Semiosis: Semiosis refers to the process of signification, the way in which signs convey meaning. It involves the interaction between signifiers and signifieds and the interpretation of signs by individuals or within a cultural context.</p> </li> <li> <p>Sign Systems: Different domains of human activity have their own unique sign systems. For example, language is a sign system, as are visual symbols like traffic signs or corporate logos. Semiotics explores these various sign systems and how they function.</p> </li> <li> <p>Semiotic Analysis: Semiotic analysis involves examining signs and symbols in their cultural and social contexts. Scholars in this field analyze how signs are used in literature, art, advertising, and everyday life to convey meaning, construct narratives, and influence behavior.</p> </li> </ul> <p>Semiotics is an interdisciplinary field with applications in linguistics, literature, anthropology, psychology, marketing, and many other areas. It helps us understand how communication works, how meanings are constructed, and how cultures shape and are shaped by signs and symbols. Prominent semioticians include Ferdinand de Saussure, Charles Sanders Peirce, and Roland Barthes.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Data%20Type/Nominal%20Data/","title":"Nominal Data","text":"<p>Definition: Nominal data represent categories or labels with no inherent order or ranking. These categories are mutually exclusive, meaning that each data point can belong to only one category. Examples: Colors</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Data%20Type/Ordinal%20Data/","title":"Ordinal Data","text":"<p>[!info] Description: Ordinal data represent categories or labels with a clear order or ranking, but the intervals between the categories are not defined or consistent. In other words, you know that one category is greater or lesser than another, but you can't say by how much.</p> <ul> <li> <p>Definition: types of categorical data used in statistics to describe and classify variables Examples: Educational attainment (e.g., high school diploma, bachelor's degree, master's degree)</p> </li> <li> <p>lack precise numerical properties.</p> </li> </ul>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Descriptive%20Statistics/","title":"Descriptive Statistics","text":"<p>Descriptive statistics are a set of techniques used to summarize and describe the main features of a dataset. These statistics provide a concise overview of the data, allowing researchers and analysts to understand its central tendencies, variations, and distribution. Descriptive statistics do not involve making inferences or drawing conclusions about a larger population; instead, they focus on characterizing the sample data itself. Here are some common descriptive statistics:</p> <ol> <li>Measures of Central Tendency:</li> <li>Mean (Average): The sum of all values in the dataset divided by the number of observations. It represents the \"typical\" value in the dataset.</li> <li>Median: The middle value in a sorted dataset. It is less sensitive to outliers than the mean.</li> <li> <p>Mode: The value(s) that occur most frequently in the dataset.</p> </li> <li> <p>Measures of Dispersion (Variability):</p> </li> <li>Range: The difference between the maximum and minimum values in the dataset. It provides an idea of the data's spread.</li> <li>Variance: A measure of how much individual data points deviate from the mean. It quantifies the spread of data.</li> <li>Standard Deviation: The square root of the variance. It measures the average deviation of data points from the mean.</li> <li> <p>Interquartile Range (IQR): The range between the first quartile (25th percentile) and the third quartile (75th percentile). It is useful for understanding the spread of data while being less affected by extreme values.</p> </li> <li> <p>Measures of Shape and Distribution:</p> </li> <li>[[Skewness]]: Indicates the asymmetry of the data distribution. Positive skewness means the distribution is skewed to the right, while negative skewness means it's skewed to the left.</li> <li> <p>[[Kurtosis]]: Measures the \"tailedness\" or peakedness of the data distribution. High kurtosis indicates heavy tails, while low kurtosis indicates light tails.</p> </li> <li> <p>Frequency Distribution:</p> </li> <li>Histogram: A graphical representation of the data's distribution, showing the frequency of values within specified bins.</li> <li> <p>Frequency Table: A tabular summary of data values along with their respective frequencies.</p> </li> <li> <p>Percentiles and Quartiles:</p> </li> <li>Percentiles: Values that divide the data into 100 equal parts (e.g., the 25th percentile is the value below which 25% of the data falls).</li> <li> <p>Quartiles: Values that divide the data into four equal parts (e.g., the first quartile is the 25th percentile, the second quartile is the median, and the third quartile is the 75th percentile).</p> </li> <li> <p>Summary Statistics:</p> </li> <li>Count: The number of observations in the dataset.</li> <li>Sum: The total of all values in the dataset.</li> <li>Mean, Median, Mode (as mentioned above): Central tendency measures.</li> <li>Variance, Standard Deviation (as mentioned above): Measures of variability.</li> </ol> <p>Descriptive statistics are an essential first step in data analysis. They provide a basic understanding of the dataset's characteristics, which can help identify trends, outliers, and patterns, guiding further analysis and decision-making.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/","title":"Mean","text":"<p>[!info] Description: The mean, often referred to as the average, is a fundamental statistical measure used to describe the central tendency of a dataset.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/#definition","title":"Definition","text":"<p>The mean is calculated by summing all the values in a dataset and then dividing that sum by the number of values in the dataset. It is represented mathematically as:</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/#common-usage","title":"Common Usage","text":"<p>The mean provides a measure of central location that is sensitive to all values in the dataset. It is particularly useful when dealing with data that follows a normal distribution or when a simple, overall summary of data is needed.</p> <p>\\(Mean (\u03bc)\\) = (Sum of all values) / (Number of values)</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/#variations","title":"Variations","text":"<p>There are different types of means, including:</p> <ul> <li>Arithmetic Mean: The standard mean calculated as described above.</li> <li>Geometric Mean: Used for exponential growth and calculating the average of rates.</li> <li>Harmonic Mean: Used for averaging rates or ratios.</li> <li>Weighted Mean: Allows assigning different weights to values based on their importance.</li> </ul>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/#limitations","title":"Limitations","text":"<p>While the mean is a powerful measure, it can be sensitive to extreme values, making it less suitable for datasets with outliers or skewed distributions. In such cases, the median may be a more appropriate measure of central tendency.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mean/#see-also","title":"See Also","text":"<ul> <li>Median</li> <li>Mode</li> </ul>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Median/","title":"Median","text":"<p>The median is a statistical measure commonly used to describe the central tendency of a dataset. It is one of the three main measures of central tendency, alongside the mean (average) and mode (most frequent value).</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Median/#definition","title":"Definition","text":"<p>The median is defined as the middle value of a dataset when it is arranged in either ascending or descending order. In cases where there is an even number of data points, the median is calculated as the average of the two middle values.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Median/#robustness","title":"Robustness","text":"<p>One of the key advantages of the median is its robustness. Unlike the mean, which is sensitive to extreme outliers, the median provides a measure of central location that is resistant to extreme values. This makes it particularly useful when dealing with skewed data or data that does not follow a normal distribution.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Median/#applications","title":"Applications","text":"<p>The median is widely used in various fields, including finance, economics, healthcare, and social sciences. It helps researchers and analysts gain insights into the typical or central value of a dataset without being unduly influenced by extreme observations.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Median/#see-also","title":"See Also","text":"<ul> <li>Mean</li> <li>Mode</li> </ul>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/","title":"Mode","text":"<p>The mode is a statistical measure used to describe the most frequently occurring value or values in a dataset.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/#definition","title":"Definition","text":"<p>The mode is the value or values that appear with the highest frequency in a dataset. Unlike the mean (average) and median (middle value), the mode is not necessarily unique; a dataset can have one mode (unimodal), more than one mode (multimodal), or no mode at all if all values occur with equal frequency.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/#common-usage","title":"Common Usage","text":"<p>The mode is particularly useful when dealing with categorical or discrete data, such as nominal or ordinal data, where it is not meaningful to calculate a traditional mean or median. In such cases, the mode helps identify the most typical category or value.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/#variations","title":"Variations","text":"<ul> <li>Unimodal: A dataset with one mode.</li> <li>Bimodal: A dataset with two modes.</li> <li>Multimodal: A dataset with more than two modes.</li> </ul>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/#limitations","title":"Limitations","text":"<p>While the mode is simple to understand and calculate, it may not provide a complete picture of a dataset, especially when dealing with continuous data or datasets with multiple modes. In such cases, additional measures like the mean and median may be used for a more comprehensive analysis.</p>"},{"location":"Statistics/Data%20Analysis%20and%20Visualization/Descriptive%20Statistics/Mode/#see-also","title":"See Also","text":"<ul> <li>Mean</li> <li>Median</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/","title":"Bernoulli Distribution","text":"<p>Desculpe pela confus\u00e3o anterior. Aqui est\u00e1 a corre\u00e7\u00e3o com cifr\u00f5es simples para as f\u00f3rmulas inline e cifr\u00f5es duplos para as f\u00f3rmulas em destaque:</p>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#bernoulli-distribution-mini-wiki","title":"Bernoulli Distribution: Mini Wiki","text":""},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#overview","title":"Overview","text":"<p>The Bernoulli Distribution is a discrete probability distribution of a random variable which takes the value 1 (success) with probability \\(p\\) and the value 0 (failure) with probability \\(q = 1 - p\\). This distribution represents the behavior of a single experiment where the outcome is binary, often termed as a \"success/failure\" or \"yes/no\" experiment.</p>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#parameters","title":"Parameters","text":"<ul> <li>\\(p\\): Probability of success on any given trial ( \\(0 \\leq p \\leq 1\\) )</li> <li>\\(q\\) or \\(1 - p\\): Probability of failure on any given trial</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>The PMF of the Bernoulli distribution is given by: $$ P(X=k) = \\begin{cases}  p &amp; \\text{if } k = 1, \\ 1 - p &amp; \\text{if } k = 0. \\end{cases} $$</p>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#expectation-and-variance","title":"Expectation and Variance","text":"<ul> <li>Expected Value (Mean): \\(\\mu = p\\)</li> <li>Variance: \\(\\sigma^2 = p(1 - p)\\)</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#applications","title":"Applications","text":"<ol> <li>Quality Control: Used in manufacturing processes to analyze the probability of defective items.</li> <li>Finance: Useful in modeling binary outcomes such as the movement of stock prices.</li> <li>Healthcare: Applied to study the probability of the presence or absence of a disease.</li> </ol>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#properties","title":"Properties","text":"<ul> <li>The Bernoulli distribution is a special case of the Binomial distribution where \\(n = 1\\).</li> <li>It can be used as a building block for other more complex probability distributions.</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#related-distributions","title":"Related Distributions","text":"<ul> <li>Binomial Distribution: Sum of \\(n\\) independent Bernoulli trials.</li> <li>Geometric Distribution: Number of Bernoulli trials needed to get one success.</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#usage-in-statistics-and-data-science","title":"Usage in Statistics and Data Science","text":"<ul> <li>Often used in logistic regression as it models binary outcome variables.</li> <li>Employed for hypothesis testing concerning proportions.</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#limitations","title":"Limitations","text":"<ul> <li>Limited to modeling experiments with only two possible outcomes.</li> </ul>"},{"location":"Statistics/Distributions/Bernoulli%20Distribution/#conclusion","title":"Conclusion","text":"<p>The Bernoulli Distribution provides a mathematical framework for analyzing experiments with binary outcomes. Its simplicity makes it a foundational concept in probability theory and statistics, serving as a starting point for understanding more complicated distributions and models.</p> <p>For deeper understanding, one can explore textbooks on probability and statistics, or reliable online educational resources.</p>"},{"location":"Statistics/Distributions/Exponential%20Family/","title":"Exponential Family","text":"<ul> <li>Normal distribution</li> <li>Exponencial</li> <li>Gamma</li> <li>Chi-Squared ^5fa6da</li> <li>Beta</li> <li>Dirichlet</li> <li>Bernoulli</li> <li>categorical</li> <li>Poisson</li> <li>Wishart</li> <li>inverse Wishart</li> <li>geometric</li> </ul>"},{"location":"Statistics/Distributions/Normal%20distribution/","title":"Normal distribution","text":"<p>aka Gaussian distribution</p> <ul> <li>continuous probability funcions</li> <li>it needs variance and mean \\(\\mathcal{N}\\sim(\\mu, \\sigma^2)\\)</li> <li></li> </ul>"},{"location":"Statistics/Distributions/Symmetry/","title":"Symmetry","text":"<p>![[Pasted image 20231016051339.png]]</p> <p>![[Pasted image 20231016051348.png]]</p> <p>![[Pasted image 20231016051353.png]]</p>"},{"location":"Statistics/Distributions/t-distribution/","title":"T distribution","text":"<p>The t-distribution, also known as the Student's t-distribution, is a probability distribution that plays a crucial role in statistics, particularly in hypothesis testing when the population standard deviation is unknown and sample sizes are relatively small. The t-distribution is similar in shape to the standard normal distribution (z-distribution) but has heavier tails, making it more appropriate for small sample sizes.</p> <p>Here are some key characteristics and points about the t-distribution:</p> <ol> <li> <p>Shape: The t-distribution is symmetric and bell-shaped, much like the normal distribution. However, it has thicker tails, which means it has more variability in the extreme values compared to the normal distribution.</p> </li> <li> <p>Parameter: The t-distribution is characterized by a single parameter called degrees of freedom (df). The degrees of freedom represent the sample size and play a crucial role in determining the shape of the distribution. As the degrees of freedom increase, the t-distribution approaches the normal distribution.</p> </li> <li> <p>Application: The t-distribution is commonly used when working with small sample sizes and estimating population parameters. It is used in situations where the population standard deviation is unknown, and sample data are used to estimate it. In such cases, the sample standard deviation is used as an estimate, and the t-distribution adjusts for the uncertainty introduced by this estimation.</p> </li> <li> <p>t-Statistic: In hypothesis testing, the t-statistic is calculated using the sample mean, population mean (or reference value), sample standard deviation, and sample size. The formula for the t-statistic is similar to the z-score formula:</p> </li> </ol> <p>[t = \\frac{{\\bar{x} - \\mu}}{{s / \\sqrt{n}}}]</p> <ul> <li>\\(\\bar{x}\\) is the sample mean.</li> <li>\\(\\mu\\) is the population mean (or reference value).</li> <li>\\(s\\) is the sample standard deviation.</li> <li> <p>\\(n\\) is the sample size.</p> </li> <li> <p>Degrees of Freedom: The degrees of freedom for the t-distribution are determined by \\(n - 1\\), where \\(n\\) is the sample size. The choice of degrees of freedom impacts the shape of the distribution. As the degrees of freedom increase, the t-distribution approaches the standard normal distribution.</p> </li> <li> <p>Critical Values: Critical values for the t-distribution are used in hypothesis testing to determine the threshold beyond which results are considered statistically significant. The critical values depend on the chosen significance level (alpha) and the degrees of freedom.</p> </li> </ul> <p>In summary, the t-distribution is a probability distribution that is particularly useful in situations where the population standard deviation is unknown, and small sample sizes are involved. It provides a way to account for the increased uncertainty associated with estimating the standard deviation from the sample data. The choice of degrees of freedom determines the specific t-distribution used in a given analysis.</p>"},{"location":"Statistics/Distributions/z-distribution/","title":"Z distribution","text":"<p>The z-distribution, also known as the standard normal distribution, is a probability distribution with a bell-shaped and symmetric curve. It plays a fundamental role in statistics and is used as a reference distribution in various statistical tests and analyses. Unlike other probability distributions, the z-distribution has a mean of 0 and a standard deviation of 1.</p> <p>Here are some key points about the z-distribution:</p> <ol> <li>Standardization: The z-distribution is used to standardize data by converting it into z-scores (standard scores). A z-score represents how many standard deviations a data point is from the mean. The formula for calculating a z-score for a data point \\(x\\) is:</li> </ol> <p>[z = \\frac{{x - \\mu}}{{\\sigma}}]</p> <ul> <li>\\(z\\) is the z-score.</li> <li>\\(x\\) is the data point.</li> <li>\\(\\mu\\) is the mean of the distribution.</li> <li> <p>\\(\\sigma\\) is the standard deviation of the distribution.</p> </li> <li> <p>Properties: The z-distribution has several important properties:</p> </li> <li>It is symmetric, with a mean (\u03bc) of 0 and a standard deviation (\u03c3) of 1.</li> <li>The area under the curve of the z-distribution between any two z-scores represents the probability of observing a data point within that range.</li> <li> <p>The z-distribution is a continuous distribution that extends from negative infinity to positive infinity.</p> </li> <li> <p>Standard Normal Table: A standard normal table, also known as the z-table, provides critical values for the standard normal distribution. It helps determine probabilities associated with specific z-scores. By looking up z-scores in the table, you can find the probability that a randomly selected data point from a standard normal distribution falls within a certain range.</p> </li> <li> <p>Z-Scores and Hypothesis Testing: In hypothesis testing and statistical analysis, z-scores are used to assess how extreme or unusual a data point or sample statistic is. Z-tests are conducted by comparing sample z-scores to critical values from the standard normal distribution to make inferences about population parameters.</p> </li> <li> <p>Central Limit Theorem (CLT): The z-distribution is related to the Central Limit Theorem (CLT), which states that the distribution of sample means from a population approaches a normal distribution as the sample size increases, even if the population itself is not normally distributed. This is important in inferential statistics when working with sample means.</p> </li> <li> <p>Characteristics: The z-distribution is characterized by a probability density function that follows the familiar bell-shaped curve, with 68% of data falling within one standard deviation from the mean, 95% within two standard deviations, and 99.7% within three standard deviations.</p> </li> </ul> <p>In summary, the z-distribution is a standard reference distribution with known properties and is widely used in statistical analysis, hypothesis testing, and inferential statistics. It provides a standardized way to assess the probability of observing data points within specific ranges and plays a crucial role in making statistical inferences.</p>"},{"location":"Statistics/Inference/Independence%20of%20Observations/","title":"Independence of Observations","text":"<p>Concept of Independence of Observations</p> <p>The concept of independence of observations is fundamental in statistics and data analysis. It refers to the condition in which the observations or measurements in a dataset are not influenced by other observations in the same dataset. In other words, independence of observations means that the value or outcome of one observation is not related to or affected by the value or outcome of another observation.</p> <p>Independence of observations is an important assumption in many statistical methods and analyses because it allows results to be more reliable and generalizable. When observations are independent, statistics and statistical tests can be applied more appropriately, and conclusions drawn from the data are more robust.</p> <p>Here are some key points related to the independence of observations:</p> <ol> <li>Examples of Independence:</li> <li>In a coin toss experiment, the outcomes of previous tosses do not influence the outcomes of future tosses. Therefore, the tosses are considered independent.</li> <li> <p>In a public opinion survey, the responses of different respondents are considered independent, as long as one respondent's opinions are not influenced by the responses of others.</p> </li> <li> <p>Dependence vs. Independence: Independence of observations is the opposite of dependence. When observations are dependent, it means that the value of one observation is related to the value of another observation, which can complicate statistical analysis.</p> </li> <li> <p>Importance in Statistics: Many statistical methods, such as calculating means, standard deviations, hypothesis tests, and regression models, assume the independence of observations. If this assumption is not met, statistical results can be biased or inaccurate.</p> </li> <li> <p>Time Series: In time series data, where data is collected over time, independence of observations may not be met, as values at adjacent time points may be correlated. In such cases, specific techniques for handling temporal dependence, such as time series models, are used.</p> </li> <li> <p>Statistical Validity: Ensuring the independence of observations is essential for the statistical validity of analyses and results. When independence is not met, more advanced statistical techniques may be needed, or the impact of dependence on results must be carefully considered.</p> </li> </ol> <p>In summary, the independence of observations is a critical concept in statistics that refers to the condition in which each observation or measurement in a dataset is not influenced by other observations, enabling more robust and generalizable statistical analyses.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Critical%20Region/","title":"Critical Region","text":"<p>The critical region, also known as the rejection region, is a concept in hypothesis testing in statistics. It represents the set of values of a test statistic that leads to the rejection of the null hypothesis. In other words, when the test statistic falls within the critical region, it indicates that the observed data is so extreme that it is unlikely to have occurred by random chance, leading to the rejection of the null hypothesis in favor of the alternative hypothesis.</p> <p>![[Pasted image 20230916194036.png]]</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Critical%20Value/","title":"Critical Value","text":"<p>A critical value is a specific value or set of values that define the boundary or threshold in a statistical hypothesis test or confidence interval. These values are used to make decisions about whether to reject or fail to reject the null hypothesis in hypothesis testing or to determine the range of values for a confidence interval.</p> <p>![[Pasted image 20230916194200.png]]</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/","title":"Hypothesis testing","text":""},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Hypothesis testing is a fundamental statistical method used to make inferences about population parameters based on sample data. It involves formulating two competing hypotheses and using statistical techniques to assess the evidence and make a decision about which hypothesis is more likely.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#definition","title":"Definition","text":"<p>In hypothesis testing, two hypotheses are typically considered:</p> <ul> <li> <p>Null Hypothesis (H0): This is the default or initial assumption. It often represents the status quo or no effect. It is the hypothesis that researchers aim to test against.</p> </li> <li> <p>Alternative Hypothesis (H1 or Ha): This is the hypothesis researchers want to support or prove. It typically represents a specific effect, difference, or relationship.</p> </li> </ul> <p>The goal of hypothesis testing is to determine whether there is enough statistical evidence to reject the null hypothesis in favor of the alternative hypothesis.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#key-steps-in-hypothesis-testing","title":"Key Steps in Hypothesis Testing","text":"<ol> <li> <p>Formulate Hypotheses: Clearly state the null hypothesis and the alternative hypothesis.</p> </li> <li> <p>Collect Data: Gather data through experiments, surveys, or observations.</p> </li> <li> <p>Choose a Significance Level (\u03b1): Determine the threshold for statistical significance, often set at 0.05 (5%).</p> </li> <li> <p>Perform a Statistical Test: Select an appropriate statistical test based on the type of data and research question. Common tests include [[t-tests]], Chi-Squared, [[ANOVA]], and [[regression analysis]].</p> </li> <li> <p>Calculate Test Statistic: Compute a test statistic that quantifies the difference between the sample data and what is expected under the null hypothesis.</p> </li> <li> <p>Determine P-Value: Calculate the p-value, which represents the probability of observing the test statistic (or more extreme results) if the null hypothesis is true.</p> </li> <li> <p>Make a Decision: Compare the p-value to the chosen significance level (\u03b1). If the p-value is less than \u03b1, reject the null hypothesis. Otherwise, fail to reject the null hypothesis.</p> </li> <li> <p>Draw Conclusions: Based on the decision, draw conclusions about the research question. If the null hypothesis is rejected, it suggests evidence in favor of the alternative hypothesis.</p> </li> </ol>"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#latex-formulas","title":"LaTeX Formulas","text":"<p>Here are some common LaTeX formulas related to hypothesis testing:</p> <ul> <li>Null Hypothesis: \\(H_0\\)</li> <li>Alternative Hypothesis: \\(H_1\\) or \\(H_a\\)</li> <li>Test Statistic: \\(T\\)</li> <li>P-Value: \\(p\\)</li> <li>Significance Level: \\(\\alpha\\)</li> </ul> <p>The decision rule can be expressed as:</p> \\[ \\begin{align*} \\text{If } p &lt; \\alpha &amp; \\text{, reject } H_0 \\\\ \\text{If } p \\geq \\alpha &amp; \\text{, fail to reject } H_0 \\end{align*} \\]"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#applications","title":"Applications","text":"<p>Hypothesis testing is widely used in various fields, including science, social sciences, medicine, and engineering, to draw conclusions about research questions, make informed decisions, and validate or refute hypotheses.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Hypothesis%20testing/#see-also","title":"See Also","text":"<ul> <li>[[Statistical Significance]</li> <li>[[Type I and Type II Errors]]</li> </ul>"},{"location":"Statistics/Inference/Hypothesis%20testing/P-Value/","title":"P Value","text":"<p>The p-value, short for \"probability value,\" is a fundamental concept in statistics, particularly in hypothesis testing. It quantifies the evidence against a null hypothesis and helps determine whether the results of a statistical test are statistically significant. The p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from sample data, assuming that the null hypothesis is true.</p> <p>Key points about the p-value:</p> <ol> <li> <p>Definition: The p-value is a probability that measures the likelihood of obtaining results as extreme as those observed, assuming that the null hypothesis is true. In other words, it tells you how unusual or rare the observed data is under the assumption of no effect.</p> </li> <li> <p>Interpretation: A smaller p-value indicates stronger evidence against the null hypothesis. Commonly used significance levels (\\(\\alpha\\)) are 0.05 (5%) and 0.01 (1%). If the p-value is less than or equal to \\(\\alpha\\), it is typically interpreted as evidence to reject the null hypothesis.</p> </li> <li> <p>Decision Rule: In hypothesis testing, you compare the calculated p-value to the chosen significance level (\\(\\alpha\\)) to make a decision. If \\(p \\leq \\alpha\\), you reject the null hypothesis; if \\(p &gt; \\alpha\\), you fail to reject it.</p> </li> <li> <p>Two-Tailed vs. One-Tailed Tests: The choice of a one-tailed or two-tailed test affects how you interpret the p-value. In a two-tailed test, the p-value represents the probability of observing results as extreme as those observed in either tail of the distribution. In a one-tailed test, it represents the probability of results as extreme as those observed in one specific direction.</p> </li> <li> <p>Comparing p-Value to Significance Level: If the p-value is smaller than or equal to the chosen significance level (\\(\\alpha\\)), you reject the null hypothesis, indicating that the results are statistically significant. If the p-value is larger than \\(\\alpha\\), you fail to reject the null hypothesis, suggesting that the results are not statistically significant.</p> </li> <li> <p>Effect Size: While the p-value helps determine statistical significance, it does not provide information about the practical or substantive significance of an effect. Effect size measures, such as Cohen's d or odds ratios, are often used to quantify the magnitude of an effect.</p> </li> <li> <p>Caution: A small p-value does not prove that a significant effect is practically meaningful or important. Researchers should consider both statistical and practical significance when interpreting results.</p> </li> <li> <p>p-Value Misinterpretation: It is essential to avoid misinterpreting p-values. A small p-value does not prove the null hypothesis is false; it only suggests that the data provides evidence against it. Also, a large p-value does not prove the null hypothesis is true; it simply indicates a lack of strong evidence against it.</p> </li> </ol> <p>In summary, the p-value is a critical tool in hypothesis testing, providing a quantified measure of evidence against the null hypothesis. Researchers use it to determine whether the results of a statistical test are statistically significant and to make informed decisions based on sample data.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Significance%20Level/","title":"Significance Level","text":"<p>The significance level, often denoted as alpha (\\(\\alpha\\)), is a critical parameter in hypothesis testing and statistical analysis. It represents the probability of making a Type I error, which is the error of rejecting a true null hypothesis. In other words, the significance level determines the threshold for statistical significance in hypothesis tests.</p> <p>Key points about the significance level (\\(\\alpha\\)):</p> <ol> <li> <p>Definition: The significance level is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from sample data, assuming that the null hypothesis is true.</p> </li> <li> <p>Common Values: Commonly used significance levels include 0.05 (5%), 0.01 (1%), and 0.10 (10%). These values represent the maximum acceptable risk of making a Type I error.</p> </li> <li> <p>Decision Rule: In hypothesis testing, the significance level determines the critical region (rejection region) of the test. If the calculated test statistic falls in the critical region, you reject the null hypothesis. The critical region is defined by critical values based on the chosen significance level.</p> </li> <li> <p>Type I Error: When you set a significance level of \\(\\alpha\\), you are controlling the risk of making a Type I error. A smaller significance level (\\(\\alpha\\)) reduces the chance of making a Type I error but increases the risk of making a Type II error (failing to detect a true effect).</p> </li> <li> <p>One-Tailed vs. Two-Tailed Tests: The significance level depends on whether you are conducting a one-tailed or two-tailed hypothesis test. For a one-tailed test, \\(\\alpha\\) is divided between one tail of the distribution (e.g., for testing in one direction). For a two-tailed test, \\(\\alpha\\) is divided evenly between both tails (e.g., for testing in either direction).</p> </li> <li> <p>Interpretation: A significance level of 0.05, for example, implies that there is a 5% chance of erroneously rejecting the null hypothesis when it is actually true. It reflects the researcher's willingness to accept a certain level of risk associated with making incorrect decisions based on sample data.</p> </li> <li> <p>Adjustment: Researchers can choose different significance levels based on the context of their study and the consequences of Type I errors. A lower significance level (e.g., 0.01) is more conservative and requires stronger evidence to reject the null hypothesis.</p> </li> <li> <p>P-Value Comparison: In some cases, researchers compare the calculated p-value to the significance level to make decisions in hypothesis testing. If the p-value is less than or equal to \\(\\alpha\\), the null hypothesis is rejected.</p> </li> </ol> <p>In summary, the significance level (\\(\\alpha\\)) is a critical parameter that helps researchers control the risk of making Type I errors in hypothesis testing. It serves as a threshold for determining whether the results of a statistical test are statistically significant and whether the null hypothesis should be rejected in favor of the alternative hypothesis.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/T-Test/","title":"T Test","text":"<p>A t-test is a statistical hypothesis test used to determine whether there is a significant difference between the means of two groups or populations. It is a parametric test that relies on certain assumptions, primarily the assumption of normality and homogeneity of variances. T-tests are widely used in various fields, including biology, psychology, economics, and many others, to compare group means and make inferences about population parameters.</p> <p>Here are the key components and types of t-tests:</p> <p>1. Two Sample T-Test (Independent Samples T-Test):    - This type of t-test is used when you want to compare the means of two independent groups to determine if there is a statistically significant difference between them.    - Example: Comparing the average test scores of two different groups of students (e.g., one group who received a specific treatment and another group who did not).</p> <p>2. Paired Sample T-Test (Dependent Samples T-Test):    - The paired sample t-test is used when you want to compare the means of two related or paired groups. It assesses whether there is a significant difference between the means of the paired observations.    - Example: Testing whether a new drug has a significant effect on blood pressure by measuring the blood pressure of the same individuals before and after treatment.</p> <p>3. One Sample T-Test:    - This t-test is used when you have a single sample and want to test if its mean is significantly different from a known or hypothesized population mean.    - Example: Testing whether the average weight of apples in a specific orchard is significantly different from a known average weight.</p> <p>Key Concepts and Steps in a T-Test:</p> <ul> <li> <p>Null Hypothesis (H0): The null hypothesis in a t-test states that there is no significant difference between the group means or that the population mean is equal to a specific value. It represents the status quo.</p> </li> <li> <p>Alternative Hypothesis (Ha): The alternative hypothesis in a t-test states that there is a significant difference between the group means. It represents the effect or difference you are trying to detect.</p> </li> <li> <p>t-Statistic: The t-statistic is calculated from the sample data and quantifies the difference between the sample means relative to the variability within the groups. It is used to assess whether the observed difference is statistically significant.</p> </li> <li> <p>Degrees of Freedom (df): The degrees of freedom in a t-test depend on the specific type of t-test being used and are used to determine the t-distribution.</p> </li> <li> <p>Critical Value or p-Value: The critical value is a threshold value used to determine whether the t-statistic is statistically significant. Alternatively, you can calculate a p-value, which represents the probability of obtaining a result as extreme as the observed result under the null hypothesis.</p> </li> <li> <p>Decision: Based on the comparison of the t-statistic to the critical value or the p-value to a significance level (alpha), you make a decision to either reject the null hypothesis (if the result is statistically significant) or fail to reject it (if the result is not statistically significant).</p> </li> </ul> <p>T-tests are powerful tools for comparing group means and are widely used in research and data analysis. However, it's important to ensure that the assumptions of normality and homogeneity of variances are met or consider alternative nonparametric tests if these assumptions are violated.</p>","tags":["parametric"]},{"location":"Statistics/Inference/Hypothesis%20testing/T-Test/#see-more","title":"See more","text":"<ul> <li>t-distribution</li> </ul>","tags":["parametric"]},{"location":"Statistics/Inference/Hypothesis%20testing/The%20Wilcoxon%20Rank-Sum%20Test/","title":"The Wilcoxon Rank Sum Test","text":"<p>The Wilcoxon Rank-Sum Test, also known as the Mann-Whitney U Test, is a nonparametric statistical test used to determine whether there is a statistically significant difference between two independent groups or samples. </p> <p>In essence, this test assesses whether the two groups have similar distributions or whether one tends to have consistently higher or lower values compared to the other. It accomplishes this by ranking all the values from both groups together, then summing the ranks of one group, and comparing this sum to the expected sum under the null hypothesis (which assumes no difference between groups).</p> <p>The Wilcoxon Rank-Sum Test is particularly useful when dealing with Ordinal Data, skewed, or non-normally distributed data and when the assumptions of parametric tests like the t-test cannot be met. It is widely used in various fields, including biology, social sciences, and clinical research, to compare groups in situations where a simple comparison of means might not be appropriate or valid.</p>","tags":["nonparametric"]},{"location":"Statistics/Inference/Hypothesis%20testing/Z-Test/","title":"Z Test","text":"<p>A z-test is a statistical hypothesis test used to determine whether the mean of a sample is significantly different from a known population mean. It's based on the standard normal distribution (also known as the z-distribution), which has a mean of 0 and a standard deviation of 1. The z-test is commonly used when you have a large sample size (typically n &gt; 30) and when the population standard deviation is known.</p> <p>Here's how a z-test works:</p> <ol> <li> <p>Formulate Hypotheses: The first step in a z-test is to formulate null (H0) and alternative (Ha) hypotheses. The null hypothesis typically states that there is no significant difference between the sample mean and the population mean, while the alternative hypothesis suggests the presence of a significant difference.</p> </li> <li> <p>Collect Data: Gather a sample from the population of interest. Ensure that the sample is random and sufficiently large.</p> </li> <li> <p>Calculate the Test Statistic (z-score): The test statistic, also known as the z-score, measures how many standard deviations the sample mean is away from the population mean. The formula for calculating the z-score is:</p> </li> </ol> <p>\\(\\(z = \\frac{{\\bar{x} - \\mu}}{{\\sigma / \\sqrt{n}}}\\)\\)</p> <ul> <li>\\(\\bar{x}\\) is the sample mean.</li> <li>\\(\\mu\\) is the population mean.</li> <li>\\(\\sigma\\) is the population standard deviation.</li> <li> <p>\\(n\\) is the sample size.</p> </li> <li> <p>Determine the Critical Region: Choose a significance level (alpha, often set at 0.05) to determine the critical region in the z-distribution. This defines the threshold beyond which the results will be considered statistically significant.</p> </li> <li> <p>Compare the Test Statistic to Critical Value: If the absolute value of the z-score calculated in step 3 falls in the critical region (i.e., exceeds the critical value(s) for the chosen significance level), you reject the null hypothesis. If it falls outside the critical region, you fail to reject the null hypothesis.</p> </li> <li> <p>Draw a Conclusion: Based on the comparison in step 5, you draw a conclusion regarding the null hypothesis. Rejecting the null hypothesis suggests that there is a significant difference between the sample mean and the population mean.</p> </li> </ul> <p>Z-tests are commonly used when you have a large enough sample size to rely on the central limit theorem, which states that the distribution of sample means approaches a normal distribution, even if the population distribution is not normal. Z-tests are also used when the population standard deviation is known or when an estimated standard deviation from the sample can be used as an approximation. If the population standard deviation is unknown and the sample size is small, the t-test is typically more appropriate.</p>"},{"location":"Statistics/Inference/Hypothesis%20testing/Z-Test/#see-more","title":"See more","text":"<ul> <li>z-distribution</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Advantages%20of%20Nonparametric%20Methods%20over%20Parametric%20Methods/","title":"Advantages of Nonparametric Methods over Parametric Methods","text":"<p>Advantages of Nonparametric Methods over Parametric Methods</p> <p>Nonparametric methods have several advantages over parametric methods in statistical analysis:</p> <ol> <li> <p>Robustness: Nonparametric methods are robust to violations of distributional assumptions. They do not assume that the data follows a specific probability distribution, making them suitable for a wide range of data types.</p> </li> <li> <p>Flexibility: Nonparametric methods can be applied to both continuous and discrete data, as well as data with unknown or complex distributions. This flexibility is particularly useful in real-world scenarios where data may not conform to theoretical distributions.</p> </li> <li> <p>Resistance to Outliers: Nonparametric methods are less sensitive to outliers compared to parametric methods that rely on means and variances. Outliers have a reduced impact on nonparametric analyses.</p> </li> <li> <p>Small Sample Sizes: Nonparametric tests can be more reliable with small sample sizes since they do not require as many data points to make valid inferences. This is especially beneficial in cases where data collection is challenging or expensive.</p> </li> <li> <p>Distribution-Free: Nonparametric methods are distribution-free, meaning they make minimal or no assumptions about the shape or characteristics of the underlying population distribution. This makes them appropriate when little is known about the data distribution.</p> </li> <li> <p>Ordinal Data: Nonparametric methods can handle ordinal data, which represents ordered categories without assuming equal intervals between categories. Parametric methods often require interval or ratio data.</p> </li> <li> <p>Non-Normal Data: When data is not normally distributed, nonparametric methods can provide valid results without the need for transformations or data manipulation.</p> </li> <li> <p>Ease of Interpretation: Nonparametric results are often easier to interpret and communicate to non-statistical audiences because they rely on ranks and medians rather than complex statistical parameters.</p> </li> </ol> <p>In summary, nonparametric methods offer greater versatility and robustness when dealing with diverse datasets and situations where parametric assumptions are not met. They are a valuable alternative for researchers and analysts looking to draw meaningful conclusions without making strong distributional assumptions.</p>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/","title":"Chi Squared Test   Application","text":""},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#application","title":"Application","text":"<ul> <li> <p>Goodness-of-Fit Test:   Used to determine if the sample data fits a specific distribution. It compares the observed frequencies to the expected frequencies of a particular categorical variable.</p> </li> <li> <p>Test of Independence:   Used to assess whether two categorical variables are independent of each other. It evaluates the dependence between the variables in a contingency table.</p> </li> <li> <p>Test of Homogeneity:   Used to determine if different samples come from the same population. It examines whether the distribution of sample characteristics is consistent across different populations.</p> </li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#case","title":"Case","text":""},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#example-of-chi-square-test","title":"Example of Chi-Square Test","text":""},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#objective","title":"Objective","text":"<p>Investigate whether there is a significant relationship between smoking habits (Smoker/Non-Smoker) and occurrence of lung cancer. ![[Pasted image 20231016050102.png]]</p>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#data-collection","title":"Data Collection","text":"<p>A sample data of 500 individuals is as follows:</p>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#contingency-table","title":"Contingency Table","text":"Lung Cancer No Lung Cancer Total Smoker 100 150 250 Non-Smoker 30 220 250 Total 130 370 500"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#calculation","title":"Calculation","text":"<ol> <li> <p>Expected Frequencies \\(E_{ij} = \\frac{(Row Total_i) \\times (Column Total_j)}{(Grand Total)}\\)</p> </li> <li> <p>Chi-Square Statistic \\(\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)    Where \\(O_{ij}\\) represents the observed frequency and \\(E_{ij}\\) represents the expected frequency. ![[Pasted image 20231016041819.png|250x200]] ![[Pasted image 20231016045919.png|250x200]]</p> </li> </ol>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#graph","title":"Graph","text":"<p>![[Pasted image 20231016050142.png]]</p> <p>![[Pasted image 20231016050345.png]]</p>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#hypotheses","title":"Hypotheses","text":"<ul> <li>Null Hypothesis (\\(H_0\\)): There is no association between smoking and lung cancer.</li> <li>Alternative Hypothesis (\\(H_1\\)): There is an association between smoking and lung cancer.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Chi%20Squared%20Test%20-%20Application/#results-and-conclusion","title":"Results and Conclusion","text":"<ul> <li>If the calculated Chi-Square value is greater than the critical value (for a specific alpha level, say 0.05), we reject the null hypothesis, concluding that there is a significant association between smoking and lung cancer.</li> <li>If the calculated Chi-Square value is less than the critical value, we fail to reject the null hypothesis, suggesting that the data does not provide enough evidence to conclude a significant association between smoking and lung cancer.</li> </ul> <p>![[Pasted image 20231016050604.png]]</p>"},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/","title":"Chi Squared","text":"<p>![[Pasted image 20231016041915.png]]</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#chi-squared-test-as-a-non-parametric-method-mini-wiki","title":"Chi-Squared Test as a Non-Parametric Method: Mini Wiki","text":"<p>The Chi-Square Test is a statistical tool utilized for analyzing categorical and discrete data, commonly applied in tests of independence and randomness. Here is a structured representation of the procedure:</p> <p>[[Procedure]]</p> <p>Chi Squared Test - Application</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#overview","title":"Overview","text":"<p>[!info] Definition: The Chi-Squared test, represented as \u03c7\u00b2 test, is a non-parametric statistical method used to determine if there is a significant association between two categorical variables in a sample. </p> <p>Being non-parametric means that it does not assume a specific distribution for the data, making it versatile and applicable to various types of categorical data. The test is widely used in research for hypothesis testing, especially in the fields of biology, marketing, sociology, and medicine.</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Observed Frequencies:    The first step involves counting the observed frequencies of occurrences in various categories or classes. These observations are arranged in a contingency table.</p> </li> <li> <p>Expected Frequencies:    Calculate the expected frequencies assuming that there is no association between the variables. The expected frequency is what you would expect if the null hypothesis were true.</p> </li> <li> <p>Calculating the Chi-Squared Statistic: $$ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} $$</p> </li> </ol> <p>Where \\(O_i\\) represents the observed frequency and \\(E_i\\) represents the expected frequency.</p> <p>Chi Squared Test - Application</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#assumptions-and-conditions","title":"Assumptions and Conditions","text":"<ul> <li> <p>[[Categorical Data]]:   The Chi-Squared test is applied on categorical (nominal or ordinal) data, not on continuous data.</p> </li> <li> <p>Sample Size:   A sufficient sample size is required, and it is suggested that the expected frequency should be 5 or more in each category.</p> </li> <li> <p>Independence of Observations:   The observations should be independent of each other, meaning that one observation should not influence another.</p> </li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#interpretation","title":"Interpretation","text":"<ul> <li>P-Value:   The result of the Chi-Squared test is usually interpreted using a p-value. If the p-value is below a threshold (commonly 0.05), the null hypothesis is rejected, indicating a significant association between the variables.</li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Chi-Squared/#conclusion","title":"Conclusion","text":"<p>The Chi-Squared test is a powerful non-parametric method to analyze categorical data, assessing relationships, and the goodness of fit in observed frequencies. Proper application and interpretation of the Chi-Squared test can provide valuable insights into the data, facilitating informed decision-making in research and various fields.</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/C%C3%A1lculo%20da%20Distribui%C3%A7%C3%A3o%20Acumulada%20Emp%C3%ADrica%20%28ECDF%29/","title":"C\u00e1lculo da Distribui\u00e7\u00e3o Acumulada Emp\u00edrica (ECDF)","text":"<p>A Distribui\u00e7\u00e3o Acumulada Emp\u00edrica (ECDF, sigla em ingl\u00eas para Empirical Cumulative Distribution Function) \u00e9 uma fun\u00e7\u00e3o estat\u00edstica que descreve a distribui\u00e7\u00e3o cumulativa dos dados em um conjunto de amostras. Ela \u00e9 usada para representar graficamente como os dados est\u00e3o distribu\u00eddos em rela\u00e7\u00e3o a uma vari\u00e1vel aleat\u00f3ria e \u00e9 uma maneira emp\u00edrica de estimar a fun\u00e7\u00e3o de distribui\u00e7\u00e3o acumulada de uma popula\u00e7\u00e3o a partir de dados amostrais.</p> <p>A ECDF \u00e9 constru\u00edda da seguinte maneira:</p> <ol> <li> <p>Ordene os dados em ordem crescente.</p> </li> <li> <p>Para cada valor de dados na amostra, calcule a propor\u00e7\u00e3o de valores iguais ou menores a ele na amostra. Isso \u00e9 feito dividindo o n\u00famero de valores menores ou iguais ao valor atual pelo tamanho total da amostra.</p> </li> <li> <p>Crie um gr\u00e1fico de dispers\u00e3o onde o eixo x representa os valores dos dados e o eixo y representa as propor\u00e7\u00f5es calculadas no passo 2.</p> </li> </ol> <p>A ECDF \u00e9 uma representa\u00e7\u00e3o acumulativa da distribui\u00e7\u00e3o dos dados e \u00e9 \u00fatil para visualizar como os dados est\u00e3o distribu\u00eddos em rela\u00e7\u00e3o a uma vari\u00e1vel aleat\u00f3ria. Ela permite que voc\u00ea identifique rapidamente caracter\u00edsticas importantes da distribui\u00e7\u00e3o, como a mediana, os quartis e a forma geral da distribui\u00e7\u00e3o.</p> <p>A ECDF \u00e9 uma ferramenta valiosa na an\u00e1lise explorat\u00f3ria de dados, especialmente quando voc\u00ea deseja entender a distribui\u00e7\u00e3o de seus dados sem fazer suposi\u00e7\u00f5es espec\u00edficas sobre a forma da distribui\u00e7\u00e3o subjacente. \u00c9 frequentemente usada em estat\u00edstica descritiva e em gr\u00e1ficos de probabilidade, como o gr\u00e1fico de probabilidade normal, que \u00e9 usado para verificar se os dados seguem uma distribui\u00e7\u00e3o normal.</p>"},{"location":"Statistics/Nonparametric%20Methods/Diferen%C3%A7as%20entre%20distribui%C3%A7%C3%B5es/","title":"Diferen\u00e7as entre distribui\u00e7\u00f5es","text":"<p>t-student = tem cauda mais pesada pois tem mais variabilidade</p> <p>quanto maior ponto perdido, maior a variabilidade</p> <p>por exemplo ![[Pasted image 20231015231431.png]]mais pontos abaixo da linha na esquerda ==&gt; cauda mais pesada no extremo esquerdo</p> <ul> <li>tem simetria --&gt; pode ser t-student</li> <li>n\u00e3o tem simetria --&gt; pode ser gamma</li> </ul> <p>kurtose : peso da cauda ![[Pasted image 20231015231807.png]]</p>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/","title":"Kolmogorov-Smirnov Test Procedure","text":"<p>[!info] Definition The Kolmogorov-Smirnov (K-S) test is utilized for continuous data to test the goodness of fit, determining whether a sample follows a specific distribution.</p>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#procedure-steps","title":"Procedure Steps:","text":""},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#1-formulate-the-hypotheses","title":"1) Formulate the Hypotheses","text":"<ul> <li>Null Hypothesis ( \\(H_0\\) ): The sample follows distribution X.</li> <li>Alternative Hypothesis ( \\(H_1\\) ): The sample does not follow distribution X.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#2-order-data-and-construct-a-table","title":"2) Order Data and Construct a Table","text":"<ul> <li>Order the data in ascending order.</li> <li>Calculate the Empirical Cumulative Distribution Function (ECDF) and the theoretical CDF.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#ecdf","title":"ECDF","text":"<p>Columns of the Table:    - \\(x\\): Value, ordered from smallest to largest.    - Order: From 1 to the sample size.    - \\(Fn(x)\\): Order/n (ECDF).    - \\(F0(x)\\): Theoretical CDF, calculate the integral of the probability density function (pdf).    - \\(|F0(x) - Fn(x)|\\): \\(D_n\\).    - \\(|F0(x) - Fn(x-)|\\): Crossed \\(D_n\\).  --&gt; subtrai o Fn da linha anterior</p>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#3-find-the-maximum-deviation-d-value","title":"3) Find the Maximum Deviation, \\(d\\), Value","text":"<ul> <li>Look for \\(d\\), i.e., the maximum value of the last two columns.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Kolmogorov%20Smirnov/#4-comparison-with-critical-values","title":"4) Comparison with Critical Values","text":"<ul> <li>Compare the calculated \\(d\\) value with the critical values given in the K-S test table, and identify the \\(p-value\\) range.</li> <li>If the \\(p-value &gt; 0.05\\), we do not reject the null hypothesis \\(H_0\\).</li> </ul> <p>Example for Uniform Distribution PDF:    - \\(PDF\\): \\((x+1)/2\\)</p>"},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/","title":"Non parametric Methods","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#non-parametric-tests","title":"Non parametric Tests","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#wilcoxon-test","title":"Wilcoxon Test","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#chi-squared-test","title":"Chi-Squared Test","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#teste-de-corrida","title":"Teste de Corrida","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#kolmogorov-smirnov-test","title":"Kolmogorov Smirnov Test","text":""},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#wilcoxon-test_1","title":"Wilcoxon Test","text":"<p>! ou </p>"},{"location":"Statistics/Nonparametric%20Methods/Non%20parametric%20tests/#sign-test","title":"Sign Test","text":""},{"location":"Statistics/Nonparametric%20Methods/Nonparametric%20Methods/","title":"Nonparametric Methods","text":"<p>Nonparametric statistics are a class of statistical methods used when the data do not meet the assumptions of parametric statistics. Parametric statistics typically assume that the data follow a specific probability distribution (usually the normal distribution) and that the parameters of this distribution are known or can be estimated from the data. Nonparametric statistics, on the other hand, make fewer or no assumptions about the underlying distribution of the data and are often used when dealing with categorical, ordinal, or skewed data.</p> <p>![[Pasted image 20231016053701.png]]</p> <p>Here are some key aspects and methods of nonparametric statistics:</p> <ol> <li> <p>Data Types: Nonparametric methods are particularly useful when dealing with non-normally distributed data, Ordinal Data (data with ordered categories but unknown intervals between them), or Nominal Data (categorical data with no inherent order). </p> </li> <li> <p>[[Ranking]]: Many nonparametric tests involve ranking the data, which transforms the original data into a more tractable form. This ranking is often based on the order of values, rather than their actual numerical values.</p> </li> <li> <p>Median: Instead of using the mean (average), nonparametric statistics often rely on the median (middle value) to describe central tendency because it is less sensitive to extreme outliers.</p> </li> <li> <p>Hypothesis testing: Nonparametric tests are used for hypothesis testing, just like parametric tests. Some common nonparametric tests include:</p> </li> <li> <p>Mann-Whitney U Test (The Wilcoxon Rank-Sum Test): Used to compare two independent groups or samples to determine if there is a statistically significant difference between them.</p> </li> <li> <p>Kruskal-Wallis Test: An extension of the Mann-Whitney test, used for comparing three or more independent groups.</p> </li> <li> <p>The Wilcoxon Rank-Sum Test: Used to compare two related samples or paired data to assess whether there is a statistically significant difference between them.</p> </li> <li> <p>[[Chi-Square Test]]: Used for analyzing the association between categorical variables in a contingency table.</p> </li> <li> <p>Friedman Test: An extension of the Wilcoxon signed-rank test used for comparing three or more related samples.</p> </li> <li> <p>Runs Test: Used to test whether a sequence of data is random or exhibits some form of systematic pattern.</p> </li> <li> <p>Goodness-of-Fit Tests: Nonparametric goodness-of-fit tests, such as the Kolmogorov-Smirnov test and the Anderson-Darling test, are used to assess whether a sample comes from a specific distribution.</p> </li> <li> <p>[[Correlation Analysis]]: Nonparametric methods like the Spearman rank correlation coefficient are used to assess the strength and direction of associations between variables when the assumptions of parametric correlation (e.g., Pearson correlation) are not met.</p> </li> <li> <p>Resampling Methods: Bootstrapping and permutation tests are nonparametric techniques that involve repeatedly resampling the data to estimate parameters or perform hypothesis tests without relying on specific distributional assumptions.</p> </li> <li> <p>Advantages: Nonparametric methods are robust and can be applied to a wide range of data types and situations, especially when assumptions about data distribution are violated.</p> </li> <li> <p>Limitations: Nonparametric tests may have less statistical power compared to their parametric counterparts when the data truly follow a specific distribution. Additionally, they may require larger sample sizes to achieve the same level of precision.</p> </li> </ol> <p>In summary, nonparametric statistics provide valuable tools for analyzing and drawing conclusions from data that do not conform to the assumptions of parametric statistics. They are particularly useful when dealing with categorical, ordinal, or non-normally distributed data and offer a robust approach to hypothesis testing and data analysis in a variety of fields, including social sciences, biology, and engineering.</p>"},{"location":"Statistics/Nonparametric%20Methods/Nonparametric%20Methods/#see-more","title":"See more","text":"<p>Advantages of Nonparametric Methods over Parametric Methods</p>"},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/","title":"FISCHER SIGN TEST","text":"<ul> <li>Data Characteristics:</li> <li>Continuous</li> <li>Independent</li> <li>Single median equal to the parameter theta</li> </ul> <p>[!info] Definition The Sign Test is a non-parametric statistical method used to analyze the median of a single sample or to compare the medians of two paired samples. </p> <p>It\u2019s particularly helpful when the assumptions of parametric tests, like t-tests, are not met (e.g., normality). Below is a comprehensive overview of the Sign Test:</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#sign-test","title":"Sign Test","text":"","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#application","title":"Application:","text":"<ul> <li>Single Sample: Testing whether the median of a single sample is equal to a specified value.</li> <li>Paired Samples: Testing whether there is a difference in the medians of two paired samples.</li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Data Type: The test uses ordinal, interval, or ratio data.</li> <li>Null Hypothesis (\\(H_0\\)): Assumes no difference in medians or that the median difference is zero.</li> <li>Alternative Hypothesis (\\(H_1\\)): Assumes a significant difference in medians or that the median difference is not zero.</li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#advantages","title":"Advantages:","text":"<ul> <li>Robust: Less sensitive to outliers and non-normal data.</li> <li>Flexibility: Suitable for small sample sizes.</li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#disadvantages","title":"Disadvantages:","text":"<ul> <li>Sensitivity: Less powerful than parametric tests like the t-test, meaning it might not detect differences that actually exist.</li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Sign%20Test/#example","title":"Example:","text":"<p>Suppose we are analyzing the effects of a training program by comparing the pre-test and post-test scores of participants.</p> <ol> <li>Hypotheses:</li> <li>\\(H_0: \\text{median difference} = 0\\)</li> <li> <p>\\(H_1: \\text{median difference} \\neq 0\\)</p> </li> <li> <p>Differences and Signs:</p> </li> <li> <p>Calculate differences (post-test - pre-test) and assign signs.</p> </li> <li> <p>Test Statistic:</p> </li> <li> <p>Count the number of positive differences.</p> </li> <li> <p>Decision:</p> </li> <li>Compare the count to a critical value or calculate the p-value to make a decision regarding the null hypothesis.</li> </ol> <p>Using the Sign Test provides a non-parametric alternative to analyze and interpret the central tendency of datasets or the difference between paired datasets.</p> <p>![[Sign Test - Step by Step]]</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/","title":"Run Test Procedure","text":"<p>Follow the steps below to execute the run test, which is used to determine whether a sequence of symbols occurs randomly and independently.</p>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#1-formulate-the-hypotheses","title":"1) Formulate the Hypotheses","text":"<ul> <li>Null Hypothesis ( \\(H_0\\) ): Symbols in the sequence occur randomly and independently, without significant run patterns.</li> <li>Alternative Hypothesis ( \\(H_1\\) ): Symbols in the sequence do not occur randomly, indicating the presence of significant run patterns.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#2-count-observed-runs","title":"2) Count Observed Runs","text":"<ul> <li>The sequence contains four runs of A and four runs of B. A run of A consists of several consecutive A's until there is a switch to B, and vice versa for a run of B.</li> <li>\\(r =\\) total number of runs, i.e., how many times it switched to each symbol.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#3-calculate-expected-runs","title":"3) Calculate Expected Runs","text":"<ul> <li>Under the null hypothesis of randomness, the expected number of runs can be calculated using the Wald-Wolfowitz formula:      \\(R_{\\text{exp}} = 1 + \\frac{2n_1n_2}{n_1 + n_2}\\)      where:<ul> <li>\\(R_{\\text{exp}} =\\) expected number of runs.</li> <li>\\(n_1 =\\) number of symbols of A.</li> <li>\\(n_2 =\\) number of symbols of B.</li> </ul> </li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#4-calculate-variance-of-r","title":"4) Calculate Variance of \\(R\\)","text":"<ul> <li>Details for calculating the variance should be included here.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#5-standardize-number-of-runs-and-obtain-z","title":"5) Standardize Number of Runs and Obtain \\(z\\)","text":"<ul> <li>Details for standardizing and obtaining \\(z\\) should be included here.</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Teste%20de%20Corrida/#6-comparison-with-critical-z-value-196","title":"6) Comparison with Critical \\(z\\) Value (1.96)","text":"<ul> <li>If \\(|z| &lt; |z_{\\text{crit}}|\\), we do not reject the hypothesis of randomness (1.96 is used assuming a 5% significance level for a two-tailed test).</li> </ul>"},{"location":"Statistics/Nonparametric%20Methods/Wilcoxon%20Test/","title":"Wilcoxon Test","text":"<p>The Wilcoxon Rank-Sum Test, also known as the Mann-Whitney U Test, is a non-parametric statistical hypothesis test used to compare two independent samples. It is used to determine if there are statistically significant differences between the two groups of an independent variable on a continuous or ordinal dependent variable.</p> <p>![[Pasted image 20231016050952.png]]</p> <p>![[Pasted image 20231016051051.png]]</p>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Wilcoxon%20Test/#characteristics-of-the-test","title":"Characteristics of the Test:","text":"<ul> <li> <p>Independence of Observations: The two samples being compared must be independent of each other. This means that the observations in one sample should not influence the observations in the other sample.</p> </li> <li> <p>[[Continuous Data]]: The data being analyzed should be at least ordinal, which means it should have a clear order. Continuous data can be ranked without any ties.</p> </li> <li> <p>Symmetry: The test assumes that the distributions of both groups are symmetrical about a common median, theta. It's important to note that the test doesn't assume that the data follows a normal distribution, just that it is symmetric about a median.</p> </li> </ul>","tags":["nonparametric"]},{"location":"Statistics/Nonparametric%20Methods/Wilcoxon%20Test/#application","title":"Application:","text":"<p>The Wilcoxon Rank-Sum Test is especially useful when the data does not meet the assumptions of a t-test, mainly the assumption of normally distributed data. The test ranks the data and then checks for statistically significant differences in the ranks between the two groups.</p> <p>To conduct the test, the data from both groups is combined and ranked together. The ranks of the data from one of the groups is then summed and compared to a distribution of summed ranks from randomly generated samples of the same sizes. This gives a test statistic which can be compared to a critical value or used to compute a p-value.</p> <p>If the p-value is below a predetermined threshold (e.g., 0.05), then the difference between the groups is deemed statistically significant.</p> <p>Note: The Wilcoxon Rank-Sum Test should not be confused with the Wilcoxon Signed-Rank Test, which is used for paired samples. The Rank-Sum Test is specifically for independent samples.</p>","tags":["nonparametric"]},{"location":"Statistics/Sampling/Population/","title":"Population","text":"<p>1. Population:</p> <ul> <li> <p>Definition: The population refers to the entire group or collection of individuals, objects, or elements that you want to study or draw conclusions about. It represents the complete set of all possible observations or data points related to a particular phenomenon.</p> </li> <li> <p>Examples: </p> </li> <li>If you're studying the heights of all adults in a country, the entire adult population of that country would constitute the population.</li> <li> <p>In a manufacturing process, the population could be all items produced on a particular assembly line.</p> </li> <li> <p>Characteristics:</p> </li> <li>The population is often large and may be too extensive to study comprehensively.</li> <li> <p>The characteristics or parameters of the population are typically of primary interest in statistical analysis. These parameters might include the population mean, variance, proportion, etc.</p> </li> <li> <p>Use in Statistics: While it's ideal to have data on the entire population, it's often impractical or impossible due to time, cost, or logistical constraints. Therefore, statisticians often work with samples.</p> </li> </ul>"},{"location":"Statistics/Sampling/Sample/","title":"Sample","text":"<p>2. Sample:</p> <ul> <li> <p>Definition: A sample is a subset of the population, selected in a systematic or random way, with the aim of representing the characteristics of the entire population. Sampling involves choosing a smaller group of observations from the population for the purpose of analysis.</p> </li> <li> <p>Examples: </p> </li> <li>In a survey, a group of 500 randomly selected adults from a country's population represents a sample.</li> <li> <p>Quality control in manufacturing might involve testing a random sample of 100 items from a larger production run.</p> </li> <li> <p>Characteristics:</p> </li> <li>Samples are typically smaller than populations but should be large enough to provide meaningful results.</li> <li> <p>Statistical techniques are used to make inferences about the population based on the data collected from the sample.</p> </li> <li> <p>Use in Statistics: Samples are used to estimate population parameters, test hypotheses, and draw conclusions about the population without having to study or measure every individual or element within it.</p> </li> </ul>"},{"location":"Statistics/Sampling/Sampling/","title":"Sampling","text":"<p>Key Concepts:</p> <ul> <li> <p>Random Sampling: To ensure that a sample is representative of the population, random sampling methods are often employed. This involves selecting individuals or items from the population in a way that each has an equal chance of being chosen.</p> </li> <li> <p>Sampling Error: The difference between a sample statistic (e.g., sample mean) and the corresponding population parameter is known as sampling error. It's important to recognize that samples may not perfectly reflect the population, and some degree of error is inherent in sampling.</p> </li> <li> <p>Inferential Statistics: The process of making inferences about a population based on the information gathered from a sample is known as inferential statistics. It involves using statistical methods to draw conclusions and make predictions.</p> </li> </ul>"},{"location":"Statistics/Time%20Series/Challenges/","title":"Challenges","text":"<p>\u251c\u2500\u2500 Challenges  \u2502 \u251c\u2500\u2500 Seasonality  \u2502 \u251c\u2500\u2500 Trends  \u2502 \u251c\u2500\u2500 Autocorrelation  \u2502 \u2514\u2500\u2500 Non-Stationarity</p>"},{"location":"Statistics/Time%20Series/Definition/","title":"Definition","text":"<p>Time Series Analysis: Definition, Significance, and Common Questions</p> <p>Time Series Analysis is a robust field of study, offering valuable insights and predictions regarding data points measured or recorded at successive points in time. Below is an organized structure that delves into the definition, significance, and frequently asked questions associated with Time Series Analysis.</p>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#1-definition","title":"1. Definition","text":"<p>[!info] Definition: A time series is a sequence of regularly recorded observations in a hierarchical domain (commonly time).</p> <p>Other hierarchical domain examples besides time: - space - dimensionality - wavelength</p> <ul> <li>Time Series Analysis: A statistical technique that deals with time series data, or trend analysis. Time series data are data points collected or recorded at a regular time interval, and Time Series Analysis involves the utilization of various methods to analyze these data, extract meaningful statistics, and other characteristics of the data.</li> </ul> <p>Types</p>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#2-significance","title":"2. Significance","text":"<ul> <li> <p>Forecasting: One of the principal applications of Time Series Analysis is forecasting. This technique is widely used in economics and financial analysis to predict future stock prices, economic indicators, or sales revenues.</p> </li> <li> <p>Signal Processing: Time Series Analysis helps in understanding the underlying structure and function of the data points, which is pivotal in signal processing and the detection of trends or cycles within the data.</p> </li> <li> <p>Anomaly Detection: With the aid of Time Series Analysis, analysts can detect any abnormality or anomaly in the time series data, which is crucial in fields like cybersecurity and fraud detection.</p> </li> </ul>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#4-applications","title":"4. Applications","text":"<p>Time Series Analysis is employed in various fields due to its versatility in dealing with time-dependent data. Below are some of its key applications:</p> <ul> <li> <p>Descriptive Measure: Time Series Analysis provides valuable descriptive measures that summarize and interpret the main characteristics of the data series. This application is beneficial in understanding the underlying patterns, trends, and variations in the data. Descriptive measures are often used in initial exploratory data analysis to guide subsequent in-depth analysis.</p> </li> <li> <p>Prediction: One of the most prominent applications of Time Series Analysis is in forecasting or prediction. Analysts use historical data to model the time series and then forecast future values. This is commonly applied in stock market predictions, weather forecasting, sales projections, and various other domains where anticipating future values is crucial.</p> </li> <li> <p>Explanation: Time Series Analysis aids in explaining the causal relationships and dependencies between different variables over time. By understanding the correlations and dynamics within the data, analysts can identify the factors influencing the observed trends and patterns, providing a basis for informed decision-making and strategy formulation.</p> </li> <li> <p>Control: In control engineering and process optimization, Time Series Analysis is employed to regulate and monitor systems. By analyzing the time series data of system inputs and outputs, controllers can adjust variables to achieve desired system performance and stability. This application is pivotal in industries like manufacturing, energy production, and automation where maintaining control over processes is essential for efficiency and safety.</p> </li> </ul>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#3-common-questions","title":"3. Common Questions","text":"<ul> <li>What are the components of time series data?</li> <li> <p>Typically, time series data are composed of four components: seasonality, trend, cyclicity, and irregularity.</p> </li> <li> <p>How does Time Series Analysis differ from other statistical techniques?</p> </li> <li> <p>Time Series Analysis specifically considers the sequential nature of data points, taking into account the time order and temporal spacing of observations.</p> </li> <li> <p>What are the challenges of Time Series Analysis?</p> </li> <li> <p>Some challenges include dealing with irregular intervals, handling missing values, and managing the noise in the data.</p> </li> <li> <p>Can Time Series Analysis be applied to non-linear trends?</p> </li> <li>While traditional Time Series Analysis techniques often assume linearity, there are advanced methods available to handle non-linear trends and patterns within the data.</li> </ul>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#5-conclusion","title":"5. Conclusion","text":"<p>Time Series Analysis is an indispensable tool for analysts and researchers dealing with temporal data. The technique is pivotal for forecasting, signal processing, and anomaly detection among other applications, facilitating the extraction of meaningful insights from time-ordered data points. Understanding the basics and challenges of Time Series Analysis is imperative for anyone looking to harness its predictive power and analytical capabilities efficiently.</p>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Definition/#see-more","title":"See more","text":"<p>Patterns</p> <p>Frequency</p> <p>Descriptive Analysis</p>","tags":["#statistics"]},{"location":"Statistics/Time%20Series/Frequency/","title":"Frequency","text":"<ol> <li>Frequency    Frequency in time series analysis refers to the number of observations or data points collected within a specific time interval. It is a measure of how often data is recorded, such as daily, monthly, or annually.<ul> <li>Weekly</li> <li>Annually</li> <li>Monthly</li> </ul> </li> </ol>"},{"location":"Statistics/Time%20Series/Non-Stationarity/","title":"Non Stationarity","text":"<p>![[Pasted image 20230927001751.png]]</p>"},{"location":"Statistics/Time%20Series/Non-Stationarity/#see-more","title":"See more","text":"<p>Definition Trends</p>"},{"location":"Statistics/Time%20Series/Transformation/","title":"Transformation","text":""},{"location":"Statistics/Time%20Series/Transformation/#5-time-series-transformation","title":"5. Time Series Transformation","text":"<p>Time Series Transformation refers to the alteration of the original values in a time series through a mathematical function. This process is typically undertaken to facilitate easier interpretation of the data or to simplify subsequent modeling efforts. </p> <ul> <li>Purpose of Transformation: </li> <li>Enhanced Interpretation: Transformation can render the time series more understandable or readable, providing insights that might not be evident in the original data. It often helps in revealing hidden patterns or trends that are crucial for data analysis.</li> <li> <p>Simplified Modeling: A transformed time series may be easier to model, as the transformation might stabilize the variance, make the series more stationary, or render the relationship between variables more linear.</p> </li> <li> <p>Common Transformations: </p> </li> <li>Log Transformation: Frequently used to stabilize variance in the presence of exponential growth. It can also make the series additive, which can be helpful in uncovering multiplicative patterns in the original data.</li> <li>Square Root &amp; Cube Root Transformation: These transformations can help in stabilizing variance in case of quadratic or cubic growth.</li> <li>Difference Transformation: Helps in removing trends and seasonality, making a non-stationary time series stationary, which is a common prerequisite for many time series modeling techniques.</li> <li> <p>Box-Cox Transformation: A family of power transformations that are effective in stabilizing variance and making the series more normally distributed.</p> </li> <li> <p>Choosing a Transformation: Selecting an appropriate transformation requires a deep understanding of the data and the specific goals of the analysis. Analysts might need to try multiple transformations and compare their effectiveness in simplifying the interpretation or modeling of the time series data. Sometimes, a combination of transformations might be necessary to achieve the desired results.</p> </li> <li> <p>Inverse Transformation: After analysis or modeling, analysts may need to convert the transformed data back to its original scale. This process, called inverse transformation, is crucial for interpreting the results in the context of the original data. Care should be taken to apply the correct inverse function to accurately revert the transformed values. </p> </li> </ul> <p>Through strategic transformation, a time series can be molded into a form that is more amenable to analysis, facilitating more accurate and insightful interpretations and predictions.</p>"},{"location":"Statistics/Time%20Series/Types/","title":"Types","text":"<ul> <li>Types:</li> </ul> Unidimensional Multidimensional Univariate Univariate Unidimensional Univariate Multidimensional Multivariate Multivariate Unidimensional Multivariate Multidimensional"},{"location":"Statistics/Time%20Series/Analysis%20Objectives/Descriptive%20Analysis/","title":"Descriptive Analysis","text":""},{"location":"Statistics/Time%20Series/Analysis%20Objectives/Descriptive%20Analysis/#descriptive-methods","title":"Descriptive Methods","text":"<ol> <li>Time Graph<ol> <li>![[Pasted image 20230915021417.png]]</li> </ol> </li> <li>Seasonality graph<ol> <li>![[Pasted image 20230915021550.png]]</li> </ol> </li> <li>Subseries graph<ol> <li>![[Pasted image 20230915021611.png]]</li> </ol> </li> <li>Multi-seasonality graph<ol> <li>![[Pasted image 20230915021623.png]]</li> </ol> </li> <li>Lag Graph<ol> <li>![[Pasted image 20230915021637.png]]</li> </ol> </li> </ol>"},{"location":"Statistics/Time%20Series/Components/Patterns/","title":"Patterns","text":"<ol> <li>Patterns</li> <li>White Noise: White noise refers to random and uncorrelated fluctuations in data, often seen as the absence of any discernible patterns or trends.</li> <li>Cyclicity: Cyclicity in time series data refers to recurring patterns or cycles that are not strictly periodic. These patterns may have varying lengths.</li> <li>Seasonality: Seasonality describes the regular, predictable patterns or variations in data that occur at fixed intervals, often related to calendar seasons.</li> <li>Trends: A trend is a long-term movement or direction in time series data, typically characterized by a consistent increase or decrease over time.</li> </ol>"},{"location":"Statistics/Time%20Series/Components/Trends/","title":"Trends","text":""},{"location":"Statistics/Time%20Series/Components/Trends/#6-trends-in-time-series-analysis","title":"6. Trends in Time Series Analysis","text":"<p>Trends in time series analysis refer to the general direction in which the data points tend to move over time. Understanding trends is crucial for accurate forecasting and insightful data analysis.</p>"},{"location":"Statistics/Time%20Series/Components/Trends/#definition-of-trends","title":"Definition of Trends","text":"<p>A trend in a time series is a persistent, long-term upwards or downwards movement in the data. It represents the underlying level at which the series generally fluctuates, disregarding short-term fluctuations or seasonality. Trends are often identified by smooth curves representing the general direction of data over time.</p>"},{"location":"Statistics/Time%20Series/Components/Trends/#types-of-trends","title":"Types of Trends","text":"<ul> <li>Upward Trend: An upward trend signifies that the data points, on average, are increasing over time.</li> <li>Downward Trend: Conversely, a downward trend indicates a general decline in the data points over time.</li> <li>Horizontal/Stationary Trend: A horizontal or stationary trend signifies that the data points remain relatively constant over time.</li> </ul>"},{"location":"Statistics/Time%20Series/Components/Trends/#identifying-trends","title":"Identifying Trends","text":"<ul> <li>Visual Inspection: Initially, trends can be identified by visually inspecting a plotted time series. A line chart can give analysts a general sense of the data movement.</li> <li>Statistical Methods: More sophisticated methods involve using statistical techniques to quantify and identify trends. These may include regression analysis, moving averages, and exponential smoothing, among others.</li> </ul>"},{"location":"Statistics/Time%20Series/Components/Trends/#detrending","title":"Detrending","text":"<ul> <li>Purpose: Detrending is the process of removing trends from a time series to analyze the cyclical and irregular components of the series.</li> <li>Methods: Various methods can be used for detrending, including differencing, regression analysis, and transformation.</li> </ul>"},{"location":"Statistics/Time%20Series/Components/Trends/#importance-of-understanding-trends","title":"Importance of Understanding Trends","text":"<p>Understanding trends in time series data is crucial for several reasons: - Forecasting: Trends provide a baseline from which future values can be predicted. - Insightful Analysis: Recognizing and understanding trends allow for more informed decision-making and planning. - Anomaly Detection: Identifying the expected trend can aid in spotting anomalies and outliers in the data.</p>"},{"location":"Statistics/Time%20Series/Components/Trends/#conclusion","title":"Conclusion","text":"<p>Recognizing and understanding trends in time series data is foundational for any analysis. Trends provide crucial context and guidance for interpreting the data, making predictions, and identifying unusual events or anomalies. Familiarity with trends allows analysts to work more effectively with time series data, deriving valuable insights and making informed decisions.</p>"}]}